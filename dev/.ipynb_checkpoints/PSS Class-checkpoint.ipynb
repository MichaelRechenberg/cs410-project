{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look to PSS folder for most up-to-date code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "# The most important package :P\n",
    "import metapy\n",
    "\n",
    "print(\"Look to PSS folder for most up-to-date code\")\n",
    "exit(1)\n",
    "\n",
    "class PSS:\n",
    "    raw_docs_dir = \"raw_docs\"\n",
    "    parsed_docs_dir = \"parsed_docs\"\n",
    "    \n",
    "    def __init__(self, root_dir, config_file, collections=None, weights={}, parser_mappings={}):\n",
    "    \n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        #List of all directories within raw_docs_dir\n",
    "        if(collections is None):\n",
    "            self.collections = os.listdir(os.path.join(self.root_dir, PSS.raw_docs_dir))\n",
    "        else:\n",
    "            self.collections = collections\n",
    "        \n",
    "        #Dictionary with (collection_name, weight) key-value pairs\n",
    "        self.weights = weights\n",
    "        \n",
    "        #Dictionary with (collection_name, parser_function) mappings\n",
    "        #Used so each collection can have a specified parser for all files in that parser\n",
    "        self.parser_mappings = {}\n",
    "        \n",
    "        #String containing asbolute path to the config.toml file\n",
    "        self.config_file = config_file\n",
    "        \n",
    "        #TODO: allow user to modify their ranker\n",
    "        self.ranker = metapy.index.OkapiBM25(k1=1.2, b = 0.75, k3=500)\n",
    "        \n",
    "        #This will be generated with parse_raw_docs()\n",
    "        self.idx = None \n",
    "        \n",
    "    def set_parser_mappings(self, mappings):\n",
    "        \"\"\"\n",
    "            Set the parser mappings \n",
    "        \"\"\"\n",
    "        self.parser_mappings = mappings\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "            Sets the weights for each collection\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "    \n",
    "        \n",
    "    def get_raw_docs_dir(self):\n",
    "        \"\"\"\n",
    "            Returns absolute path to the raw_docs directory\n",
    "        \"\"\"\n",
    "        return os.path.join(self.root_dir, PSS.raw_docs_dir)\n",
    "    \n",
    "    def get_parsed_docs_dir(self):\n",
    "        \"\"\"\n",
    "            Returns absolute path to the parsed_docs directory\n",
    "        \"\"\"\n",
    "        return os.path.join(self.root_dir, PSS.parsed_docs_dir)\n",
    "\n",
    "    def _make_one_line(self, parsed_file):\n",
    "        \"\"\"\n",
    "            Makes a file into one line, separated by spaces\n",
    "            Note this destroys the original parsed file\n",
    "            \n",
    "            Params: \n",
    "                parsed_file - absolute path to the file within parsed_docs_dir we want to \n",
    "                    make into one line\n",
    "        \"\"\"\n",
    "        one_line = \"\"\n",
    "        with open(parsed_file, \"r\") as file:\n",
    "            one_line = \" \".join([line.strip() for line in file])\n",
    "        \n",
    "        os.remove(parsed_file)\n",
    "        \n",
    "        with open(parsed_file, \"w+\") as file:\n",
    "            file.write(one_line + '\\n')\n",
    "    \n",
    "    def parse_raw_docs(self):\n",
    "        \"\"\"\n",
    "            Parse all of the raw docs, put them in the parsed_docs_dir, \n",
    "                and fill out the metadata.dat file with the file paths as well as\n",
    "                the parsed_docs-full-corpus.txt with \n",
    "                    [none] relative_filename\n",
    "                blocks\n",
    "        \"\"\"\n",
    "        print(\"Beginning Parsing\")\n",
    "\n",
    "        #parsed_docs-full-corpus.txt\n",
    "        #order in this file determines what docID's are\n",
    "        full_corpus_filename = os.path.join(self.get_parsed_docs_dir(), \"{0}-full-corpus.txt\".format(PSS.parsed_docs_dir))\n",
    "        \n",
    "        #metadata.dat file\n",
    "        metadata_filename = os.path.join(self.get_parsed_docs_dir(), \"metadata.dat\")\n",
    "        \n",
    "        with open(full_corpus_filename, \"w+\") as full_corpus_file:\n",
    "            with open(metadata_filename, \"w+\") as metadata_file:\n",
    "\n",
    "                for collection in self.collections:\n",
    "                    files = [f for f in os.listdir(os.path.join(self.get_raw_docs_dir(), collection))]\n",
    "                    # Select the parser fucntion for this collection\n",
    "                    parser_func = self.parser_mappings[collection]\n",
    "                    for raw_file in files:\n",
    "                        abs_raw_filename = os.path.join(os.path.join(self.get_raw_docs_dir(), collection), \n",
    "                                                        raw_file)\n",
    "                        #user defined parser function that will write whatever\n",
    "                        #  text representation they want into the PSS.parsed_docs_dir\n",
    "                        #note that the name of the file written to PSS.parse_docs_dir\n",
    "                        #  must be the same as the file that lives in PSS.raw_docs_dir\n",
    "                        #  (for .pdf you should still write the_file.pdf as the file name\n",
    "                        #   within PSS.raw_docs_dir even though the parsed file isn't \n",
    "                        #   actually a .pdf file...metapy treats it all like a text document)\n",
    "                        parser_func(abs_raw_filename, self.get_parsed_docs_dir())\n",
    "                        #write entry in metadata file\n",
    "                        relative_to_public_doc_path = os.path.join(collection, raw_file)\n",
    "                        metadata_file.write(\"{0}\\n\".format(relative_to_public_doc_path))\n",
    "                        full_corpus_file.write(\"[none] {0}\\n\".format(raw_file))\n",
    "                        #self._make_one_line(os.path.join(self.get_parsed_docs_dir(), raw_file))\n",
    "\n",
    "        print(\"Finished Parsing\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_index(self):\n",
    "        \"\"\"\n",
    "            Generate index based off the files in parsed_docs folder\n",
    "            Call parse_raw_docs() before calling this function\n",
    "        \"\"\"\n",
    "        self.idx = metapy.index.make_inverted_index(self.config_file)\n",
    "        \n",
    "    \n",
    "    def score_query(self, user_query, num_returned):\n",
    "        \"\"\"\n",
    "            Ranks all documents within the PSS's inverted index and\n",
    "                weights scores wrt the collection weights\n",
    "                \n",
    "            user_query - string containing the query you want to search the index with\n",
    "            num_returned - integer representing how many results you want returned\n",
    "                Note: you may have less than num_returned results returned\n",
    "                \n",
    "            Returns:\n",
    "                A list of document paths (relative to their location in the raw_docs directory)\n",
    "                    of the top num_returned documents\n",
    "        \"\"\"\n",
    "        query = metapy.index.Document()\n",
    "        query.content(user_query)\n",
    "        # Init our PSS_Ranker so we can apply the collection weights\n",
    "        pss_ranker = PSS_Ranker(self)\n",
    "        results = pss_ranker.score(self.idx, query, num_returned)\n",
    "        top_doc_paths = []\n",
    "        for result in results:\n",
    "            top_doc_paths.append(self.idx.metadata(result[0]).get(\"doc_path\"))\n",
    "        return top_doc_paths\n",
    "    \n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "class PSS_Ranker(metapy.index.RankingFunction):\n",
    "    \"\"\"\n",
    "        Custom RankingFunction that applies a typical ranking \n",
    "            function (BM25, Dirichlet Prior) to score a document\n",
    "            and then multiplies that score by the collection\n",
    "            weight of the document\n",
    "    \"\"\"\n",
    "    def __init__(self, pss_object):\n",
    "        \"\"\"\n",
    "            pss_object - the PSS object with its index and weights initialized\n",
    "        \"\"\"\n",
    "        self.pss_object = pss_object\n",
    "        super(PSS_Ranker, self).__init__()\n",
    "\n",
    "    def score_one(self, sd):\n",
    "        # Score given by a 'real' ranker\n",
    "        # TODO: have this ranker passed in from PSS class\n",
    "        ranker = metapy.index.OkapiBM25()\n",
    "        raw_score = ranker.score_one(sd)\n",
    "        doc_id = sd.d_id\n",
    "        doc_path = self.pss_object.idx.metadata(doc_id).get(\"doc_path\")\n",
    "        collection_name = os.path.dirname(doc_path)\n",
    "        weight = self.pss_object.weights[collection_name]\n",
    "        return raw_score * weight\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "# example dumb parsers we can use\n",
    "# all parsers take the form parser(filename, parsed_docs_directory)\n",
    "#   filename - absolute filename of a file within the raw_docs directory\n",
    "#      Note: you can extract the base filename with os.path.basename(filename)\n",
    "#   parsed_docs_directory - absolute path of directory used to store the parsed docs\n",
    "def stupid_parse_file(filename, parsed_docs_directory):\n",
    "    \"\"\"\n",
    "        Params:\n",
    "            filename -- the absolute path to the file within raw_docs we want to parse\n",
    "            parsed_docs_directory -- asbolute path of the directory where we will store the parsed docs\n",
    "\n",
    "        Dummy parser which just adds RITA RITA RITA as the last line\n",
    "            and writes the parsed file to parsed_docs_directory\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\") as raw_file:\n",
    "        parsed_filename = os.path.join(parsed_docs_directory, os.path.basename(filename))\n",
    "        with open(parsed_filename, \"w+\") as parsed_file:\n",
    "            parsed_file.write('RITA RITA RITA\\n')\n",
    "            for line in raw_file:\n",
    "                parsed_file.write(line)\n",
    "            parsed_file.write('RITA RITA RITA\\n')\n",
    "        \n",
    "    \n",
    "def stupid_parse_file2(filename, parsed_docs_directory):\n",
    "    \"\"\"\n",
    "        Params:\n",
    "            filename -- the absolute path to the file within raw_docs we want to parse\n",
    "            parsed_docs_directory -- asbolute path of the directory where we will store the parsed docs\n",
    "\n",
    "        Dummy parser which just adds MP MP MP as the last line\n",
    "            and writes the parsed file to parsed_docs_directory\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, \"r\") as raw_file:\n",
    "        parsed_filename = os.path.join(parsed_docs_directory, os.path.basename(filename))\n",
    "        with open(parsed_filename, \"w+\") as parsed_file:\n",
    "            parsed_file.write('MP MP MP MP\\n')\n",
    "            for line in raw_file:\n",
    "                parsed_file.write(line)\n",
    "            parsed_file.write('MP MP MP MP\\n')\n",
    "            \n",
    "            \n",
    "#Some not so dumb example parsers\n",
    "\n",
    "def copy_parser_function(filename, parsed_docs_directory):\n",
    "    \"\"\"\n",
    "        Simple parser function that simply copies the file to the parsed_docs directory,\n",
    "            unmodified\n",
    "        If you want to not modify the raw file at all, use this function\n",
    "        Using line by line and not shutil.copyfile() because we don't want to lose metadata\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as raw_file:\n",
    "        parsed_filename = os.path.join(parsed_docs_directory, os.path.basename(filename))\n",
    "        with open(parsed_filename, \"w+\") as parsed_file:\n",
    "            for line in raw_file:\n",
    "                parsed_file.write(line)\n",
    "\n",
    "def html_parser_function(filename, parsed_docs_directory):\n",
    "    \"\"\"\n",
    "        HTML parser function that extracts the text from certain tags\n",
    "            (using Beautiful Soup) and writes the text fro each tag on\n",
    "            it's own line\n",
    "    \"\"\"\n",
    "    tags = ['p', 'h1', 'h2', 'h3', 'title']\n",
    "    with open(filename, \"r\") as htmlfile:\n",
    "        soup = BeautifulSoup(htmlfile, \"html5lib\")\n",
    "        parsed_filename = os.path.join(parsed_docs_directory, os.path.basename(filename))\n",
    "        with open(parsed_filename, \"w+\") as parsed_file:\n",
    "            for tag in tags:\n",
    "                found_tags = soup.find_all(tag)\n",
    "                for found_tag in found_tags:\n",
    "                    parsed_file.write(\"{0}\\n\".format(found_tag.text))\n",
    "                    \n",
    "def pdf_parser_function(filename, parsed_docs_directory):\n",
    "        \n",
    "    #If the user isn't on a linux machine or pdftotext isn't installed they'll have to to generate\n",
    "    #  the text representation via some external mechanism and store\n",
    "    #  that representation within the parsed_docs directory\n",
    "    #The metadata information and parsed_docs-full-corpus.txt will still be filled\n",
    "    #  out correctly if the user adds their processed PDF into the parsed_docs\n",
    "    #  directory without modifying the file's name\n",
    "    \n",
    "    parsed_filename = os.path.join(parsed_docs_directory, os.path.basename(filename))\n",
    "    print(\"Note: PSS's PDF Parser only works for Linux\")\n",
    "    # Use pdftotext command, so this only works on linux\n",
    "    os.system(\"pdftotext {0} {1}\".format(filename, parsed_filename))\n",
    "\n",
    "\n",
    "def pptx_parser_function(filename, parsed_docs_directory):\n",
    "    print(\"No pptx parser yet :/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy_parser_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40ebcbd6e848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#  a specific collection is parsed a specific way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparser_mappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mparser_mappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'header_files'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_parser_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mparser_mappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mp_docs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstupid_parse_file2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparser_mappings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'angrave_book'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_parser_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy_parser_function' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# We create a PSS object\n",
    "cwd = os.getcwd()\n",
    "a = PSS(cwd, os.path.join(cwd, \"config.toml\"))\n",
    "# We set the parser mappings for each collection so every file within\n",
    "#  a specific collection is parsed a specific way\n",
    "parser_mappings = {}\n",
    "parser_mappings['header_files'] = copy_parser_function\n",
    "parser_mappings['mp_docs'] = stupid_parse_file2\n",
    "parser_mappings['angrave_book'] = html_parser_function\n",
    "parser_mappings['pdfs'] = pdf_parser_function\n",
    "a.set_parser_mappings(parser_mappings)\n",
    "# We set weights for each collection\n",
    "# These weights will be multiplied to the score given by a\n",
    "#   'real' ranker like BM25/Dirichlet Prior in order to give\n",
    "#   documents from one collection a higher score than documents\n",
    "#   from other collections\n",
    "weights = {}\n",
    "weights['angrave_book'] = 1\n",
    "weights['header_files'] = 1\n",
    "weights['mp_docs'] = 1\n",
    "weights['pdfs'] = 1\n",
    "a.set_weights(weights)\n",
    "a.parse_raw_docs()\n",
    "a.generate_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1d686e0e77a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Note we don't have to regenerate the index to do this, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#  These can be set on a per-query basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'angrave_book'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# Oh wait, we like angrave's book so let's update his weight\n",
    "# Note we don't have to regenerate the index to do this, so\n",
    "#  These can be set on a per-query basis\n",
    "a.set_weights(weights)\n",
    "weights['angrave_book'] = 5\n",
    "\n",
    "results = a.score_query(\"split apply combine\", 20)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
